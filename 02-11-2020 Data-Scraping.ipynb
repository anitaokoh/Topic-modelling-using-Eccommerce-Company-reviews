{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Source through scrapping\n",
    "This Includes getting the neccessary urls, using request to get the HTML format,parsing with BeautifulSoup and finally converting data lists to dataframe and then storing in CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "5PqRTJCi_gEK"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate strings to get all the full path Urls needed\n",
    "This includes\n",
    "- Identifying the 3 company url needed, the trustpilot root url and the additives for page changes\n",
    "- Concating the company url and the trustpilot rool url to form a home review url for each of the companys' pages\n",
    "- concating the urls for other pages on the each company's review page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "fjLqqIHTIaWl"
   },
   "outputs": [],
   "source": [
    "company_page  = ['www.zalando.de','outfittery.de', 'www.amazon.de']\n",
    "root_url = 'https://www.trustpilot.com/review/'\n",
    "additive = '?page='"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "1vWSZFoMEdF5"
   },
   "outputs": [],
   "source": [
    "def form_other_pages_url(home_url,additive,count_range,company_url_list=[]):\n",
    "    \"\"\"Get the urls for other pages\"\"\"\n",
    "  for count in count_range:\n",
    "    if count == 1:\n",
    "        other_page_link = home_url\n",
    "    else:\n",
    "        other_page_link = home_url + additive + str(count)\n",
    "    company_url_list.append(other_page_link)\n",
    "  return company_url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "PAPQAyF3I9GQ"
   },
   "outputs": [],
   "source": [
    "def get_url(count_range,company_url):\n",
    "    \"\"\"Get the home url as well as the other pages url for each company and store in a list\"\"\"\n",
    "  home_url = root_url + company_url\n",
    "  company_list = form_other_pages_url(home_url,additive,count_range,company_url_list=[])\n",
    "  return company_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "m-HqyjxuKmYw"
   },
   "outputs": [],
   "source": [
    "zalando_urls = get_url(range(1,6),company_page[0])\n",
    "amazon_urls = get_url(range(1,26),company_page[2])\n",
    "outfittery_urls = get_url(range(1,6),company_page[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store and Parse HTML Content\n",
    "This Includes\n",
    "- Getting the HTML body from all the URls\n",
    "- Parsing and getting all tags contents\n",
    "- Store the text formats into lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "83mvMFlTRHB9"
   },
   "outputs": [],
   "source": [
    "def get_reviews_objects(url):\n",
    "    \"\"\"Get the html bodies of the url and parse to beautifulsoup\"\"\"\n",
    "    page_url= requests.get(url).text\n",
    "    soup = BeautifulSoup(page_url, 'lxml')\n",
    "    review_title = soup.find_all('a',class_='link link--large link--dark')\n",
    "    review_body = soup.find_all('p',class_='review-content__text')\n",
    "    return review_title,review_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Mj7tOV3cNQON"
   },
   "outputs": [],
   "source": [
    "def url_list (url_list):\n",
    "    \"\"\"Call the get_review objects function and return the titles and bodies of the reviews. Sore in seperate lists\"\"\"\n",
    "  apprend_content = []\n",
    "  title_bulk = []\n",
    "  body_bulk = []\n",
    "  for url in url_list:\n",
    "     review_title,review_body = get_reviews_objects(url)\n",
    "     title_text = [review.text for review in review_title]\n",
    "     body_text = [review.text for review in review_body]\n",
    "     title_bulk += title_text\n",
    "     body_bulk += body_text\n",
    "\n",
    "  apprend_content.append([title_bulk,body_bulk])\n",
    "  return apprend_content\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "jND790_EQj3S"
   },
   "outputs": [],
   "source": [
    "zalando_content = url_list(zalando_urls)\n",
    "amazon_content = url_list(amazon_urls)\n",
    "outfittery_content = url_list(outfittery_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store Data in DataFrame and map each review to their company origins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "TkummC2KUYSu"
   },
   "outputs": [],
   "source": [
    "data_base = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "3ogYhPlJUxKl"
   },
   "outputs": [],
   "source": [
    "map_company = {0:'zalando',1:'Amazon',2:'Outfittery'}\n",
    "content_list = [zalando_content,amazon_content,outfittery_content]\n",
    "\n",
    "for index,content in enumerate(content_list):\n",
    "  data = pd.DataFrame(zip(content[0][0],content[0][1]))\n",
    "  data['company'] = index\n",
    "  data_base = pd.concat([data_base,data],ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "s8Ix6pajX6O-"
   },
   "outputs": [],
   "source": [
    "data_base['company'] = data_base['company'].map(map_company)\n",
    "data_base.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "UGhfGw42YHHg"
   },
   "outputs": [],
   "source": [
    "data_base.to_csv('trustpilot_reviews.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FkDY3y51ZqZy"
   },
   "source": [
    "The data is then stored in a csv for further processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "02-10-2020 Data-Scraping.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
